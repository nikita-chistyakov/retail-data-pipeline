{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO58onlHR7RBPS+JT0Ufp9D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# AI is at the forefront of many companies effort to drive innovation and growth.\n","But to get started with any of these, we need data to be in the right place, in the right shape, at the right time.\n","\n","ETL (extract and transform before loading) :\n","- traditional and common way\n","- tabular and non-tabular data\n","- Python `pandas`\n","\n","ELT (extract and load before transforming) ðŸ‡°\n","- more recent\n","- data warehouse\n","- tabular data"],"metadata":{"id":"9iEzFO8CP5dh"}},{"cell_type":"markdown","source":["# ETL pipeline:\n","Using three custom-built Python functions to extract, transform, and load data. The load function, which writes a pandas DataFrame to a SQL database. Then, the extract, transform, and load functions are called to execute the ETL pipeline."],"metadata":{"id":"MyanTeSbZsYZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwGu-ZYZPzc-"},"outputs":[],"source":["def load(data_frame, target_table):\n","  # Some custom-built Python logic to load data to `SQL`\n","  data_frame.to_sql(name=target_table, con=POSTGRES_CONNECTION)\n","  print(f\"Loading data to the {target_table} table\")\n","\n","# Now, run the data pipeline\n","extracted_data = extract(file_name=\"raw_data.csv\")\n","transformed_data = transform(data_frame=extracted_data)\n","load(data_frame=transformed_data, target_table=\"cleaned_data\")"]},{"cell_type":"code","source":["# Extract data from the raw_data.csv file\n","extracted_data = extract(file_name=\"raw_data.csv\")\n","\n","# Transform the extracted_data\n","transformed_data = transform(data_frame=extracted_data)\n","\n","# Load the transformed_data to cleaned_data.csv\n","load(data_frame=transformed_data, target_table=\"cleaned_data\")"],"metadata":{"id":"inWoUhVEeNvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load(data_frame, file_name):\n","  # Write cleaned_data to a `CSV` using file_name\n","  data_frame.to_csv(file_name)\n","  print(f\"Successfully loaded data to {file_name}\")\n","\n","extracted_data = extract(file_name=\"raw_data.csv\")\n","\n","# Transform extracted_data using transform() function\n","transformed_data = transform(data_frame=extracted_data)\n","\n","# Load transformed_data to the file transformed_data.csv\n","load(data_frame=transformed_data, file_name=\"transformed_data.csv\")\n"],"metadata":{"id":"a1Da9bV4pzY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Complete building the transform() function\n","def transform(source_table, target_table):\n","  data_warehouse.execute(f\"\"\"\n","  CREATE TABLE {target_table} AS\n","      SELECT\n","          CONCAT(\"Product ID: \", product_id),\n","          quantity * price\n","      FROM {source_table};\n","  \"\"\")\n","\n","extracted_data = extract(file_name=\"raw_sales_data.csv\")\n","load(data_frame=extracted_data, table_name=\"raw_sales_data\")\n","\n","# Populate total_sales by transforming raw_sales_data\n","transform(source_table=\"raw_sales_data\", target_table=\"total_sales\")"],"metadata":{"id":"YqZNXIxmqAKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read a CSV with a path stored using file_name into memory\n","def extract(file_name):\n","  return pd.read_csv(file_name)\n","\n","# Filter the data_frame to only incude a subset of columns\n","def transform(data_frame):\n","  return data_frame.loc[:, [\"industry_name\", \"number_of_firms\"]]\n","\n","# Write the data_frame to a CSV\n","def load(data_frame, file_name):\n","  data_frame.to_csv(file_name)\n","\n","extracted_data = extract(file_name=\"raw_industry_data.csv\")\n","transformed_data = transform(data_frame=extracted_data)\n","\n","# Pass the transformed_data DataFrame to the load() function\n","load(data_frame=transformed_data, file_name=\"number_of_firms.csv\")s"],"metadata":{"id":"CCWYEa1dqhAW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ELT pipeline:\n"],"metadata":{"id":"QhoXcA0vab4k"}},{"cell_type":"code","source":["def transform (source_table, target_table):\n","data_warehouse.run_sql(\"\"\"\n","  CREATE TABLE {target_table} AS\n","    SELECT\n","      <field-name>, <field-name>,\n","    FROM {source_table};\n","\"\"\"\n","\n","# Similar to ETL pipelines, call the extract, load, and transform functions\n","extracted_data = extract(file_name=\"raw_data.csv\")\n","load(data_frame=extracted_data, table_name=\"raw_data\")\n","transform(source_table=\"raw_data\", target_table=\"cleaned_data\")"],"metadata":{"id":"2neg_VFbacEP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extracting data from structured sources\n","Almost all data pipelines are initiated by extracting data from a source system. Most common source systems are:\n","- CSV, parquet*, and JSON files (*Apache Parquet is an open-source, column-oriented file format designed for efficient field storage and retrieval)\n","- dynamic stores such as SQL databases\n","- APIs, which are commonly used to ingest data from a third party\n","- within organizations, data lakes and warehouses are popular source systems for data pipelines\n","- in more advanced pipelines, it's even common to web scrape the data."],"metadata":{"id":"qFc39EaZxari"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Read the sales data (parquet file) into a DataFrame\n","sales_data = pd.read_parquet(\"sales_data.parquet\", engine=\"fastparquet\")\n","\n","# Check the data type of the columns of the DataFrames\n","print(sales_data.dtypes)\n","\n","# Print the shape of the DataFrame, as well as the head\n","print(sales_data.shape)\n","print(sales_data.head())"],"metadata":{"id":"v95te9ujxa-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pulling data from SQL databases\n","import sqlalchemy\n","\n","# Create a connection to the sales database\n","db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/sales\")\n","\n","# Query the sales table\n","raw_sales_data = pd.read_sql(\"sales\", db_engine)\n","print(raw_sales_data)"],"metadata":{"id":"V9fa_8ln0q_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code modularization to help make pipelines more readable and reusable, and can help to expedite troubleshooting efforts.\n","def extract():\n","  \t# Create a connection URI and connection engine\n","    connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/sales\"\n","    db_engine = sqlalchemy.create_engine(connection_uri)"],"metadata":{"id":"JcYXLL0w1RLV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract():\n","    connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/sales\"\n","    db_engine = sqlalchemy.create_engine(connection_uri)\n","    raw_data = pd.read_sql(\"SELECT * FROM sales WHERE quantity_ordered = 1\", db_engine)\n","\n","    # Print the head of the DataFrame\n","    print(raw_data.head())\n","\n","    # Return the extracted DataFrame\n","    return raw_data\n","\n","# Call the extract() function\n","raw_sales_data = extract()"],"metadata":{"id":"DXoj3B_d2em-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transforming data with pandas\n"],"metadata":{"id":"FM0udjAg6hzV"}},{"cell_type":"code","source":["# Extract data from the sales_data.parquet path\n","raw_sales_data = extract(\"sales_data.parquet\")\n","\n","def transform(raw_data):\n","  \t# Only keep rows with `Quantity Ordered` greater than 1\n","    clean_data = raw_data.loc[raw_data['Quantity Ordered'] > 1, :]\n","\n","    # Only keep columns \"Order Date\", \"Quantity Ordered\", and \"Purchase Address\"\n","    clean_data = clean_data.loc[:, [\"Order Date\", \"Quantity Ordered\", \"Purchase Address\"]]\n","\n","    # Return the filtered DataFrame\n","    return clean_data\n","\n","transform(raw_sales_data)"],"metadata":{"id":"bR9Vusby6irW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_sales_data = extract(\"sales_data.csv\")\n","\n","def transform(raw_data):\n","    # Convert the \"Order Date\" column to type datetime\n","    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n","\n","    # Only keep items under ten dollars\n","    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n","    return clean_data\n","\n","clean_sales_data = transform(raw_sales_data)\n","\n","# Check the data types of each column\n","print(clean_sales_data.dtypes)"],"metadata":{"id":"eGwm6B80-Wkk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Validating data transformations"],"metadata":{"id":"qgqXU7G1_-nI"}},{"cell_type":"code","source":["raw_sales_data = extract(\"sales_data.csv\")\n","\n","def transform(raw_data):\n","    # Convert the \"Order Date\" column to type datetime\n","    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n","\n","    # Only keep items under ten dollars\n","    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n","    return clean_data\n","\n","clean_sales_data = transform(raw_sales_data)\n","\n","# Check the data types of each column\n","print(clean_sales_data.dtypes)\n","\n","\n","# What is the value of the price \"Price Each\" column of the two most expensive items in the transformed DataFrame?\n","print(clean_sales_data[\"Price Each\"].sort_values(ascending=False))"],"metadata":{"id":"by4vs1NI_-ux"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading sales data to a CSV file\n","Loading data is an essential component of any data pipeline. It ensures that any data consumers and processes have reliable access to data that I've extracted and transformed earlier in a pipeline."],"metadata":{"id":"D64zfqS1CpOt"}},{"cell_type":"code","source":["def transform(raw_data):\n","\t# Find the items prices less than 25 dollars\n","\treturn raw_data.loc[raw_data[\"Price Each\"] < 25, [\"Order ID\", \"Product\", \"Price Each\", \"Order Date\"]]\n","\n","def load(clean_data):\n","\t# Write the data to a CSV file without the index column\n","\tclean_data.to_csv(\"transformed_sales_data.csv\", index=False)\n","\n","\n","clean_sales_data = transform(raw_sales_data)\n","\n","# Call the load function on the cleaned DataFrame\n","load(clean_sales_data)"],"metadata":{"id":"JeQ9QnPOCpaa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Customizing a CSV file\n","Sometimes, data needs to be stored in a CSV file in a customized manner. This may include using different header values, including or excluding the index column of a DataFrame, or altering the character used to separate columns."],"metadata":{"id":"WYXC4MuySSMX"}},{"cell_type":"code","source":["# Import the os library\n","import os\n","\n","# Load the data to a csv file with the index, no header and pipe separated\n","def load(clean_data, path_to_write):\n","\tclean_data.to_csv(path_to_write, header=False, sep=\"|\")\n","\n","load(clean_sales_data, \"clean_sales_data.csv\")\n","\n","# Check that the file is present.\n","file_exists = os.path.exists(\"clean_sales_data.csv\")\n","print(file_exists)"],"metadata":{"id":"5KSSjl3rSSWO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Persisting data to files\n","Loading data to a final destination is one of the most important steps of a data pipeline. I'll use the `transform()` function to transform product sales data before loading it to a .csv file. This will give downstream data consumers a better view into total sales across a range of products."],"metadata":{"id":"i2sO74__TnHx"}},{"cell_type":"code","source":["def load(clean_data, file_path):\n","    # Write the data to a file\n","    clean_data.to_csv(file_path, header=False, index=False)\n","\n","    # Check to make sure the file exists\n","    file_exists = os.path.exists(file_path)\n","    if not file_exists:\n","        raise Exception(f\"File does NOT exists at path {file_path}\")\n","\n","# Load the transformed data to the provided file path\n","load(clean_sales_data, \"transformed_sales_data.csv\")"],"metadata":{"id":"7lMrF5_eTnQV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Persisting data in an ETL pipeline\n","Loading data to a file is a great way to ensure that data consumers, including data scientists and analysts, have stable access to extracted and transformed data.\n","\n","While typically most visible during the \"load\" portion of an ETL process, data persistence is a best practice that can, and should, happen at multiple stages in a data pipeline.\n","\n","Persisting data to a file allows for a \"snapshot\" to be taken at various points throughout the pipeline. This is especially useful when recovering from a failure, and is essential if data is hard to reacquire from a source system."],"metadata":{"id":"uLoabQTUU9rW"}},{"cell_type":"markdown","source":["# Logging within a data pipeline\n","logging is used to alert engineers of data pipeline performance.\n","\n","Logs are messages created and written during the execution of a pipeline. They are configured by the developing party, and document the performance of a pipeline.\n","\n","Logs provide a starting point when solutions fail by letting Data Engineers revisit the execution of the pipeline.\n","\n","Logs are the foundation for all monitoring and alerting efforts, and are essential for creating transparent data pipelines."],"metadata":{"id":"lqWq-TkpW2a5"}},{"cell_type":"code","source":["def transform(raw_data):\n","    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n","    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n","\n","    # Create an info log regarding transformation\n","    logging.info(\"Transformed 'Order Date' column to type 'datetime'.\")\n","\n","    # Create debug-level logs for the DataFrame before and after filtering\n","    logging.debug(f\"Shape of the DataFrame before filtering: {raw_data.shape}\")\n","    logging.debug(f\"Shape of the DataFrame after filtering: {clean_data.shape}\")\n","\n","    return clean_data\n","\n","clean_sales_data = transform(raw_sales_data)"],"metadata":{"id":"vcLx92LvU91M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Handling exceptions when loading data\n","Sometimea data pipelines might throw an exception. These exceptions are a form of alerting, and they let a Data Engineer know when something unexpected happened. It's important to properly handle these exceptions."],"metadata":{"id":"xV6vS07GX0xd"}},{"cell_type":"code","source":["def extract(file_path):\n","    return pd.read_parquet(file_path)\n","\n","# Update the pipeline to include a try block\n","try:\n","\t# Attempt to read in the file\n","    raw_sales_data = extract(\"sales_data.parquet\")\n","\n","# Catch the FileNotFoundError\n","except FileNotFoundError as file_not_found:\n","\t# Write an error-level log\n","\tlogging.error(file_not_found)"],"metadata":{"id":"D6BlwTEtXztR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ingesting JSON data with pandas"],"metadata":{"id":"vGi7cB6nYQ06"}},{"cell_type":"code","source":["def extract(file_path):\n","  # Read the JSON file into a DataFrame\n","  return pd.read_json(file_path, orient=\"records\")\n","\n","# Call the extract function with the appropriate path, assign to raw_testing_scores\n","raw_testing_scores = extract(\"testing_scores.json\")\n","\n","# Output the head of the DataFrame\n","print(raw_testing_scores.head())"],"metadata":{"id":"RzvCBgfUYQ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reading JSON data into memory"],"metadata":{"id":"wQu4HNbPkL3O"}},{"cell_type":"code","source":["# Import the json library\n","import json\n","\n","def extract(file_path):\n","    with open(file_path, \"r\") as json_file:\n","        # Load the data from the JSON file\n","        raw_data = json.load(json_file)\n","    return raw_data\n","\n","raw_testing_scores = extract(\"nested_scores.json\")\n","\n","# Print the raw_testing_scores\n","print(raw_testing_scores)\n"],"metadata":{"id":"4lAlTYoVkMVn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Iterating over dictionaries (JSON)"],"metadata":{"id":"Fvzm5OsVkxtB"}},{"cell_type":"code","source":["raw_testing_scores_keys = []\n","\n","# Iterate through the `keys` of the raw_testing_scores dictionary\n","for school_id in raw_testing_scores.keys():\n","  \t# Append each key to the raw_testing_scores_keys list\n","\traw_testing_scores_keys.append(school_id)\n","\n","print(raw_testing_scores_keys[0:3])\n"],"metadata":{"id":"8KKLebSlkx3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_testing_scores_values = []\n","\n","# Iterate through `the values` of the raw_testing_scores dictionary\n","for school_info in raw_testing_scores.values():\n","\traw_testing_scores_values.append(school_info)\n","\n","print(raw_testing_scores_values[0:3])\n"],"metadata":{"id":"n0S6UmfvUfDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loop through both `the keys and values`\n","\n","raw_testing_scores_keys = []\n","raw_testing_scores_values = []\n","\n","# Iterate through the values of the raw_testing_scores dictionary\n","for school_id, school_info in raw_testing_scores.items():\n","\traw_testing_scores_keys.append(school_id)\n","\traw_testing_scores_values.append(school_info)\n","\n","print(raw_testing_scores_keys[0:3])\n","print(raw_testing_scores_values[0:3])"],"metadata":{"id":"MfR6YMVzUmg2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Parsing data from dictionaries"],"metadata":{"id":"i_caMMDmU4W0"}},{"cell_type":"code","source":["# Parse the street_address from the dictionary\n","street_address = school.get(\"street_address\")\n","\n","# Parse the scores dictionary\n","scores = school.get(\"scores\")\n","\n","# Parse the math, reading and writing values from scores\n","math_score = scores.get(\"math\")\n","reading_score = scores.get(\"reading\")\n","writing_score = scores.get(\"writing\")\n","\n","print(f\"Street Address: {street_address}\")\n","print(f\"Math: {math_score}, Reading: {reading_score}, Writing: {writing_score}\")\n"],"metadata":{"id":"XB5rr4w8U4hW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normalized_testing_scores = []\n","\n","# Loop through each of the dictionary key-value pairs\n","for school_id, school_info in raw_testing_scores.items():\n","\tnormalized_testing_scores.append([\n","    \tschool_id,\n","    \tschool_info.get(\"street_address\"),  # Pull the \"street_address\"\n","    \tschool_info.get(\"city\"),\n","    \tschool_info.get(\"scores\").get(\"math\", 0),\n","    \tschool_info.get(\"scores\").get(\"reading\", 0),\n","    \tschool_info.get(\"scores\").get(\"writing\", 0),\n","    ])\n","\n","print(normalized_testing_scores)"],"metadata":{"id":"4iJfDfwnV-Rx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transforming and cleaning DataFrames"],"metadata":{"id":"RBCDdI8wVwoo"}},{"cell_type":"code","source":["# Create a DataFrame from the normalized_testing_scores list\n","normalized_data = pd.DataFrame(normalized_testing_scores)\n","\n","# Set the column names\n","normalized_data.columns = [\"school_id\", \"street_address\", \"city\", \"avg_score_math\", \"avg_score_reading\", \"avg_score_writing\"]\n","\n","normalized_data = normalized_data.set_index(\"school_id\")\n","print(normalized_data.head())"],"metadata":{"id":"KXm3PJIcVww9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Filling missing values with pandas"],"metadata":{"id":"7Vr_pfOwWUUq"}},{"cell_type":"code","source":["# Print the head of the `raw_testing_scores` DataFrame\n","print(raw_testing_scores.head())"],"metadata":{"id":"u1w2WHvJWUfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fill NaN values with the average from that column\n","raw_testing_scores[\"math_score\"] = raw_testing_scores[\"math_score\"].fillna(raw_testing_scores[\"math_score\"].mean())\n","\n","# Print the head of the raw_testing_scores DataFrame\n","print(raw_testing_scores.head())\n"],"metadata":{"id":"pPYfbKs0cY1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def transform(raw_data):\n","\traw_data.fillna(\n","    \tvalue={\n","\t\t\t# Fill NaN values with column mean\n","\t\t\t\"math_score\": raw_data[\"math_score\"].mean(),\n","\t\t\t\"reading_score\": raw_data[\"reading_score\"].mean(),\n","\t\t\t\"writing_score\": raw_data[\"writing_score\"].mean()\n","\t\t}, inplace=True\n","\t)\n","\treturn raw_data\n","\n","clean_testing_scores = transform(raw_testing_scores)\n","\n","# Print the head of the clean_testing_scores DataFrame\n","print(clean_testing_scores.head())"],"metadata":{"id":"pD8mzBjdcie5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Grouping data with pandas"],"metadata":{"id":"9ZyVsns3dAKa"}},{"cell_type":"code","source":["def transform(raw_data):\n","\t# Use .loc[] to only return the needed columns\n","\traw_data = raw_data.loc[:, [\"city\", \"math_score\", \"reading_score\", \"writing_score\"]]\n","\n","    # Group the data by city, return the grouped DataFrame\n","\tgrouped_data = raw_data.groupby(by=[\"city\"], axis=0).mean()\n","\treturn grouped_data\n","\n","# Transform the data, print the head of the DataFrame\n","grouped_testing_scores = transform(raw_testing_scores)\n","print(grouped_testing_scores.head())\n"],"metadata":{"id":"j6NKc3T_dAUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Applying advanced transformations to DataFrames\n"],"metadata":{"id":"EHDxvCWiieZC"}},{"cell_type":"code","source":["def transform(raw_data):\n","\t# Use the apply function to extract the street_name from the street_address\n","    raw_data[\"street_name\"] = raw_data.apply(\n","   \t\t# Pass the correct function to the apply method\n","        find_street_name,\n","        axis=1\n","    )\n","    return raw_data\n","\n","# Transform the raw_testing_scores DataFrame\n","cleaned_testing_scores = transform(raw_testing_scores)\n","\n","# Print the head of the cleaned_testing_scores DataFrame\n","print(cleaned_testing_scores.head())\n"],"metadata":{"id":"U_eDL8o5ieho"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading data to a Postgres database"],"metadata":{"id":"2kES12eZoC-i"}},{"cell_type":"code","source":["# Update the connection string, create the connection object to the schools database\n","db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/schools\")\n","\n","# Write the DataFrame to the scores table\n","cleaned_testing_scores.to_sql(\n","\tname=\"scores\",\n","\tcon=db_engine,\n","\tindex=False,\n","\tif_exists=\"replace\"\n",")\n"],"metadata":{"id":"ycyTEIO_oDNG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Validating data loaded to a Postgres Database"],"metadata":{"id":"jyDaHdkGoIf6"}},{"cell_type":"code","source":["def load(clean_data, con_engine):\n","\t# Store the data in the schools database\n","    clean_data.to_sql(\n","    \tname=\"scores_by_city\",\n","\t\tcon=con_engine,\n","\t\tif_exists=\"replace\",  # Make sure to replace existing data\n","\t\tindex=True,\n","\t\tindex_label=\"school_id\"\n","    )\n"],"metadata":{"id":"x0zhoxgYoIp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load(clean_data, con_engine):\n","    clean_data.to_sql(name=\"scores_by_city\", con=con_engine, if_exists=\"replace\", index=True, index_label=\"school_id\")\n","\n","# Call the load function, passing in the cleaned DataFrame\n","load(cleaned_testing_scores, db_engine)\n","\n","# Call query the data in the scores_by_city table, check the head of the DataFrame\n","to_validate = pd.read_sql(\"SELECT * FROM scores_by_city\", con=db_engine)\n","print(to_validate.head())\n"],"metadata":{"id":"N96Nc4puosqh"},"execution_count":null,"outputs":[]}]}